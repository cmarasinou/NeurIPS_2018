@article{Kasiske2013,
abstract = {The Scientific Registry of Transplant Recipients is charged with providing program-specific reports for organ transplant programs in the United States. Monitoring graft survival for pancreas transplant programs has been problematic as there are three different pancreas transplant procedures that may have different outcomes, and analyzing them separately reduces events and statistical power. We combined two consecutive 2.5-year cohorts of transplant recipients to develop Cox proportional hazards models predicting outcomes, and tested these models in the second 2.5-year cohort. We used separate models for 1- and 3-year graft and patient survival for each transplant type: simultaneous pancreas-kidney (SPK), pancreas after kidney (PAK) and pancreas transplant alone (PTA). We first built a predictive model for each pancreas transplant type, and then pooled the transplant types within centers to compare total observed events with total predicted events. Models for 1-year pancreas graft and patient survival yielded C statistics of 0.65 (95{\%} confidence interval, 0.63-0.68) and 0.66 (0.61-0.72), respectively, comparable to C statistics for 1-year patient and graft survival for other organ transplants. Model calibration (Hosmer-Lemeshow method) was also acceptable. We conclude that pooling the results of SPK, PAK and PTA can produce potentially useful models for reporting program-specific pancreas transplant outcomes.},
archivePrefix = {arXiv},
arxivId = {1805.08671},
author = {Kasiske, B. L. and Gustafson, S. and Salkowski, N. and Stock, P. G. and Axelrod, D. A. and Kandaswamy, R. and Sleeman, E. F. and Wainright, J. and Israni, A. K. and Snyder, J. J.},
doi = {10.1111/ajt.12036},
eprint = {1805.08671},
file = {:Users/cmarasinou/Documents/Reading/NIPS/7688-adding-one-neuron-can-eliminate-all-bad-local-minima.pdf:pdf},
isbn = {1600-6143 (Electronic)$\backslash$r1600-6135 (Linking)},
issn = {16006135},
journal = {American Journal of Transplantation},
keywords = {Pancreas after kidney,Scientific Registry of Transplant Recipients,pancreas transplant alone,simultaneous pancreas-kidney transplant},
number = {2},
pages = {337--347},
pmid = {23289524},
title = {{Optimizing the program-specific reporting of pancreas transplant outcomes}},
url = {http://arxiv.org/abs/1805.08671},
volume = {13},
year = {2013}
}
@article{Bansal2018,
abstract = {This paper seeks to answer the question: as the (near-) orthogonality of weights is found to be a favorable property for training deep convolutional neural networks, how can we enforce it in more effective and easy-to-use ways? We develop novel orthogonality regularizations on training deep CNNs, utilizing various advanced analytical tools such as mutual coherence and restricted isometry property. These plug-and-play regularizations can be conveniently incorporated into training almost any CNN without extra hassle. We then benchmark their effects on state-of-the-art models: ResNet, WideResNet, and ResNeXt, on several most popular computer vision datasets: CIFAR-10, CIFAR-100, SVHN and ImageNet. We observe consistent performance gains after applying those proposed regularizations, in terms of both the final accuracies achieved, and faster and more stable convergences. We have made our codes and pre-trained models publicly available: https://github.com/nbansal90/Can-we-Gain-More-from-Orthogonality.},
archivePrefix = {arXiv},
arxivId = {1810.09102},
author = {Bansal, Nitin and Chen, Xiaohan and Wang, Zhangyang},
eprint = {1810.09102},
file = {:Users/cmarasinou/Documents/Reading/NIPS/7680-can-we-gain-more-from-orthogonality-regularizations-in-training-deep-networks.pdf:pdf},
number = {NeurIPS},
pages = {1--11},
title = {{Can We Gain More from Orthogonality Regularizations in Training Deep CNNs?}},
url = {http://arxiv.org/abs/1810.09102},
year = {2018}
}
@article{Wang2018,
abstract = {We replace the output layer of deep neural nets, typically the softmax function, by a novel interpolating function. And we propose end-to-end training and testing algorithms for this new architecture. Compared to classical neural nets with softmax function as output activation, the surrogate with interpolating function as output activation combines advantages of both deep and manifold learning. The new framework demonstrates the following major advantages: First, it is better applicable to the case with insufficient training data. Second, it significantly improves the generalization accuracy on a wide variety of networks. The algorithm is implemented in PyTorch, and code will be made publicly available.},
archivePrefix = {arXiv},
arxivId = {1802.00168},
author = {Wang, Bao and Luo, Xiyang and Li, Zhen and Zhu, Wei and Shi, Zuoqiang and Osher, Stanley J.},
eprint = {1802.00168},
file = {:Users/cmarasinou/Documents/Reading/NIPS/7355-deep-neural-nets-with-interpolating-function-as-output-activation.pdf:pdf},
number = {Nips},
title = {{Deep Neural Nets with Interpolating Function as Output Activation}},
url = {http://arxiv.org/abs/1802.00168},
year = {2018}
}
@article{Ma2018,
author = {Ma, Lin},
file = {:Users/cmarasinou/Documents/Reading/NIPS/7313-deep-non-blind-deconvolution-via-generalized-low-rank-approximation.pdf:pdf},
number = {NeurIPS},
title = {{Deep Non-Blind Deconvolution via Generalized Low-Rank Approximation}},
year = {2018}
}
@article{Gabrie2018,
abstract = {We examine a class of deep learning models with a tractable method to compute information-theoretic quantities. Our contributions are three-fold: (i) We show how entropies and mutual informations can be derived from heuristic statistical physics methods, under the assumption that weight matrices are independent and orthogonally-invariant. (ii) We extend particular cases in which this result is known to be rigorously exact by providing a proof for two-layers networks with Gaussian random weights, using the recently introduced adaptive interpolation method. (iii) We propose an experiment framework with generative models of synthetic datasets, on which we train deep neural networks with a weight constraint designed so that the assumption in (i) is verified during learning. We study the behavior of entropies and mutual informations throughout learning and conclude that, in the proposed setting, the relationship between compression and generalization remains elusive.},
archivePrefix = {arXiv},
arxivId = {1805.09785},
author = {Gabri{\'{e}}, Marylou and Manoel, Andre and Luneau, Cl{\'{e}}ment and Barbier, Jean and Macris, Nicolas and Krzakala, Florent and Zdeborov{\'{a}}, Lenka},
doi = {10.1016/0960-1686(92)90180-S},
eprint = {1805.09785},
file = {:Users/cmarasinou/Documents/Reading/NIPS/7453-entropy-and-mutual-information-in-models-of-deep-neural-networks.pdf:pdf},
issn = {09601686},
number = {Nips},
pages = {1--11},
title = {{Entropy and mutual information in models of deep neural networks}},
url = {http://arxiv.org/abs/1805.09785},
year = {2018}
}
@article{Guo2018,
abstract = {Understanding and interpreting how machine learning (ML) models make decisions have been a big challenge. While recent research has proposed various technical approaches to provide some clues as to how an ML model makes individual predictions, they cannot provide users with an ability to inspect a model as a complete entity. In this work, we propose a novel technical approach that augments a Bayesian non-parametric regression mixture model with multiple elastic nets. Using the enhanced mixture model, we can extract generalizable insights for a target model through a global approximation. To demonstrate the utility of our approach, we evaluate it on different ML models in the context of image recognition. The empirical results indicate that our proposed approach not only outperforms the state-of-the-art techniques in explaining individual decisions but also provides users with an ability to discover the vulnerabilities of the target ML models.},
archivePrefix = {arXiv},
arxivId = {1811.03422},
author = {Guo, Wenbo and Huang, Sui and Tao, Yunzhe and Xing, Xinyu and Lin, Lin},
eprint = {1811.03422},
file = {:Users/cmarasinou/Documents/Reading/NIPS/7703-explaining-deep-learning-models-a-bayesian-non-parametric-approach.pdf:pdf},
isbn = {8471624451},
number = {NeurIPS},
title = {{Explaining Deep Learning Models - A Bayesian Non-parametric Approach}},
url = {http://arxiv.org/abs/1811.03422},
year = {2018}
}
@article{Lindenbaum2018,
author = {Lindenbaum, Ofir and Stanley, Jay S and Wolf, Guy and Krishnaswamy, Smita},
doi = {10.1074/jbc.M710139200},
file = {:Users/cmarasinou/Documents/Reading/NIPS/7414-geometry-based-data-generation.pdf:pdf},
journal = {Physical Review Letters},
number = {23},
pages = {230402},
title = {{Geometry Based Data Generation{\_}{\_}{\_}Supplemental}},
volume = {120},
year = {2018}
}
@article{Du2018,
author = {Du, Simon S and Wang, Yining},
file = {:Users/cmarasinou/Documents/Reading/NIPS/7320-how-many-samples-are-needed-to-estimate-a-convolutional-neural-network.pdf:pdf},
number = {Nips},
title = {{How Many Samples are Needed to Estimate a Convolutional Neural Network ?}},
year = {2018}
}
@article{Garipov2018,
abstract = {The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet.},
archivePrefix = {arXiv},
arxivId = {1802.10026},
author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
doi = {10.1164/rccm.201203-0508OC},
eprint = {1802.10026},
file = {:Users/cmarasinou/Documents/Reading/NIPS/8095-loss-surfaces-mode-connectivity-and-fast-ensembling-of-dnns.pdf:pdf},
isbn = {1535-4970 (Electronic)$\backslash$r1073-449X (Linking)},
issn = {20297564},
number = {Nips},
pages = {1--10},
pmid = {23144331},
title = {{Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs}},
url = {http://arxiv.org/abs/1802.10026},
year = {2018}
}
@article{Dubey2018,
author = {Dubey, Abhimanyu and Gupta, Otkrist and Raskar, Ramesh and Naik, Nikhil},
file = {:Users/cmarasinou/Documents/Reading/NIPS/7344-maximum-entropy-fine-grained-classification.pdf:pdf},
number = {iii},
title = {{Maximum Entropy Fine-Grained Classification}},
year = {2018}
}
@article{Tao2018,
author = {Tao, Yunzhe},
file = {:Users/cmarasinou/Documents/Reading/NIPS/7331-nonlocal-neural-networks-nonlocal-diffusion-and-nonlocal-modeling.pdf:pdf},
number = {Nips},
title = {{Nonlocal Neural Networks , Nonlocal Diffusion and Nonlocal Modeling}},
year = {2018}
}
@article{Yeh2018,
abstract = {We propose to explain the predictions of a deep neural network, by pointing to the set of what we call representer points in the training set, for a given test point prediction. Specifically, we show that we can decompose the pre-activation prediction of a neural network into a linear combination of activations of training points, with the weights corresponding to what we call representer values, which thus capture the importance of that training point on the learned parameters of the network. But it provides a deeper understanding of the network than simply training point influence: with positive representer values corresponding to excitatory training points, and negative values corresponding to inhibitory points, which as we show provides considerably more insight. Our method is also much more scalable, allowing for real-time feedback in a manner not feasible with influence functions.},
archivePrefix = {arXiv},
arxivId = {1811.09720},
author = {Yeh, Chih-Kuan and Kim, Joon Sik and Yen, Ian E. H. and Ravikumar, Pradeep},
eprint = {1811.09720},
file = {:Users/cmarasinou/Documents/Reading/NIPS/8141-representer-point-selection-for-explaining-deep-neural-networks.pdf:pdf},
number = {Nips},
title = {{Representer Point Selection for Explaining Deep Neural Networks}},
url = {http://arxiv.org/abs/1811.09720},
year = {2018}
}
@article{Adebayo2018,
abstract = {Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.},
archivePrefix = {arXiv},
arxivId = {1810.03292},
author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
eprint = {1810.03292},
file = {:Users/cmarasinou/Documents/Reading/NIPS/8160-sanity-checks-for-saliency-maps.pdf:pdf},
number = {Nips},
title = {{Sanity Checks for Saliency Maps}},
url = {http://arxiv.org/abs/1810.03292},
year = {2018}
}
@article{Hou2018,
abstract = {Recently, adversarial erasing for weakly-supervised object attention has been deeply studied due to its capability in localizing integral object regions. However, such a strategy raises one key problem that attention regions will gradually expand to non-object regions as training iterations continue, which significantly decreases the quality of the produced attention maps. To tackle such an issue as well as promote the quality of object attention, we introduce a simple yet effective Self-Erasing Network (SeeNet) to prohibit attentions from spreading to unexpected background regions. In particular, SeeNet leverages two self-erasing strategies to encourage networks to use reliable object and background cues for learning to attention. In this way, integral object regions can be effectively highlighted without including much more background regions. To test the quality of the generated attention maps, we employ the mined object regions as heuristic cues for learning semantic segmentation models. Experiments on Pascal VOC well demonstrate the superiority of our SeeNet over other state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1810.09821},
author = {Hou, Qibin and Jiang, Peng-Tao and Wei, Yunchao and Cheng, Ming-Ming},
eprint = {1810.09821},
file = {:Users/cmarasinou/Documents/Reading/NIPS/7336-self-erasing-network-for-integral-object-attention.pdf:pdf},
isbn = {1810.09821v1},
number = {NeurIPS},
pages = {1--11},
title = {{Self-Erasing Network for Integral Object Attention}},
url = {http://arxiv.org/abs/1810.09821},
year = {2018}
}
@article{Yang2016,
abstract = {We use differential equations based approaches to provide some {\{}$\backslash$it $\backslash$textbf{\{}physics{\}}{\}} insights into analyzing the dynamics of popular optimization algorithms in machine learning. In particular, we study gradient descent, proximal gradient descent, coordinate gradient descent, proximal coordinate gradient, and Newton's methods as well as their Nesterov's accelerated variants in a unified framework motivated by a natural connection of optimization algorithms to physical systems. Our analysis is applicable to more general algorithms and optimization problems {\{}$\backslash$it $\backslash$textbf{\{}beyond{\}}{\}} convexity and strong convexity, e.g. Polyak-$\backslash$L ojasiewicz and error bound conditions (possibly nonconvex).},
archivePrefix = {arXiv},
arxivId = {1612.02803},
author = {Yang, Lin F. and Arora, R. and Braverman, V. and Zhao, Tuo},
doi = {arXiv:1612.02803v5},
eprint = {1612.02803},
file = {:Users/cmarasinou/Documents/Reading/NIPS/7690-the-physical-systems-behind-optimization-algorithms.pdf:pdf},
number = {Nips},
pages = {1--10},
title = {{The Physical Systems Behind Optimization Algorithms}},
url = {http://arxiv.org/abs/1612.02803},
year = {2016}
}
@article{Pennington2018,
abstract = {An important factor contributing to the success of deep learning has been the remarkable ability to optimize large neural networks using simple first-order optimization algorithms like stochastic gradient descent. While the efficiency of such methods depends crucially on the local curvature of the loss surface, very little is actually known about how this geometry depends on network architecture and hyperparameters. In this work, we extend a recently-developed framework for studying spectra of nonlinear random matrices to characterize an important measure of curvature, namely the eigenvalues of the Fisher information matrix. We focus on a single-hidden-layer neural network with Gaussian data and weights and provide an exact expression for the spectrum in the limit of infinite width. We find that linear networks suffer worse conditioning than nonlinear networks and that nonlinear networks are generically non-degenerate. We also predict and demonstrate empirically that by adjusting the nonlinearity, the spectrum can be tuned so as to improve the efficiency of first-order optimization methods.},
author = {Pennington, Jeffrey and Brain, Google and Worah, Pratik and Research, Google},
file = {:Users/cmarasinou/Documents/Reading/NIPS/7786-the-spectrum-of-the-fisher-information-matrix-of-a-single-hidden-layer-neural-network.pdf:pdf},
journal = {Neural Information Processing Systems (NIPS)},
number = {Nips},
title = {{The Spectrum of the Fisher Information Matrix of a Single-Hidden-Layer Neural Network}},
year = {2018}
}
@article{Alvarez-Melis2018,
abstract = {Objectives: The aim of this study was to compare the fracture strength of three techniques used to re-attach tooth fragments in sound and endodontically treated fractured teeth with or without fiber post placement. Material and methods: Ninety human lower incisors were randomly divided into three groups of 30 teeth each. In group A teeth were not subjected to endodontic treatment; while teeth from groups B and C were endodontically treated and the pulp chamber restored with a composite resin. All teeth were fractured by an axial load applied to the buccal area in order to obtain tooth fragments. Teeth from each group were then divided into three subgroups, according to the re-attachment technique: bonded-only, buccal-chamfer and circumferential chamfer. Before the re-attachment procedures, fiber posts were placed in teeth from group C using dual cure resin luting cement (Duo-Link). All teeth (groups A-C) had the fragments re-attached using a same dual cure resin luting cement. In the bonded-only group, no additional preparation was made. After re-attachment of the fragment, teeth from groups buccal and circumferential chamfer groups had a 1.0 mm depth chamfer placed in the fracture line either on buccal surfaceor along the buccal and lingual surfaces, respectively. Increments of microhybid composite resin (Tetric Ceram) were used in subgroups buccal chamfer and circumferential chamfer to restore the chamfer. The specimens were loaded until fracture in the same pre-determined area. The force required to detach each fragment was recorded and the data was subjected to a three-way analysis of variance where factors Group and Re-attachment technique are independent measures and Time of fracture is a repeated measure factor (first and second) and Tukey's test ($\alpha$ = 0.05). Results: The main factors Re-attachment technique (p = 0.04) and Time of fracture (p = 0.02) were statistically significant. The buccal and circumferential chamfer techniques were statistically similar (p {\textgreater} 0.05) and superior to the bonded-only group (p {\textless} 0.05). The first time of fracture was statistically superior to second time of fracture (p {\textless} 0.001). Conclusions: The use of fiber post is not necessary for the reinforcement of the tooth structure in re-attachment of endodontically treated teeth. When bonding a fractured fragment, the buccal or circumferential re-attachment techniques should be preferable in comparison with the simple re-attachment without any additional preparation. None of the techniques used for re-attachment restored the fracture strength of the intact teeth. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1806.07538},
author = {Alvarez-Melis, David and Jaakkola, Tommi S.},
doi = {10.1016/j.jdent.2008.01.001},
eprint = {1806.07538},
file = {:Users/cmarasinou/Documents/Reading/NIPS/8003-towards-robust-interpretability-with-self-explaining-neural-networks.pdf:pdf},
isbn = {0300-5712},
issn = {03005712},
number = {NeurIPS},
pmid = {18308450},
title = {{Towards Robust Interpretability with Self-Explaining Neural Networks}},
url = {http://arxiv.org/abs/1806.07538},
year = {2018}
}
@article{Wang2018a,
abstract = {It is widely believed that learning good representations is one of the main reasons for the success of deep neural networks. Although highly intuitive, there is a lack of theory and systematic approach quantitatively characterizing what representations do deep neural networks learn. In this work, we move a tiny step towards a theory and better understanding of the representations. Specifically, we study a simpler problem: How similar are the representations learned by two networks with identical architecture but trained from different initializations. We develop a rigorous theory based on the neuron activation subspace match model. The theory gives a complete characterization of the structure of neuron activation subspace matches, where the core concepts are maximum match and simple match which describe the overall and the finest similarity between sets of neurons in two networks respectively. We also propose efficient algorithms to find the maximum match and simple matches. Finally, we conduct extensive experiments using our algorithms. Experimental results suggest that, surprisingly, representations learned by the same convolutional layers of networks trained from different initializations are not as similar as prevalently expected, at least in terms of subspace match.},
archivePrefix = {arXiv},
arxivId = {1810.11750},
author = {Wang, Liwei and Hu, Lunjia and Gu, Jiayuan and Wu, Yue and Hu, Zhiqiang and He, Kun and Hopcroft, John},
doi = {arXiv:1810.11750v1},
eprint = {1810.11750},
file = {:Users/cmarasinou/Documents/Reading/NIPS/8167-towards-understanding-learning-representations-to-what-extent-do-different-neural-networks-learn-the-same-representation.pdf:pdf},
number = {NeurIPS},
pages = {1--10},
title = {{Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation}},
url = {http://arxiv.org/abs/1810.11750},
year = {2018}
}
