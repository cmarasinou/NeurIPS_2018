Authors study the similarity of representations learned by two networks with identical structure and different initializations, focusing on the same layer in each network. The representations are defined to be the activation vectors of neurons (outputs of a neuron across all samples). Similarity is defined in terms of the minimum distance between activation vectors of the first layer and linear combinations of activation vectors of the second. Similarity in subsets of the layers is examined, with the introduction of definitions describing matching between the subsets,  e.g. simple match. Efficient algorithms implementing the presented theory are given. Experiments on VGG, Resnet models with CIFAR10, ImageNet datasets are conducted. Findings show that similarity of the convolutional layers is very low in general, although high close to the input and output layers. The latter is justified by the tendency of the model to align with the data in order to produce good accuracy. It is also showed that similarity is found most on large subsets of the layers rather than on a few or individual neurons.
